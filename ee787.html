<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>EE787: Machine Learning </title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Jong-Han Kim</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="topics.html">Topics</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="ase7005.html">ASE7005&nbsp;Vehicle&nbsp;guidance</a></div>
<div class="menu-item"><a href="ase2030.html">ASE2030&nbsp;Linear&nbsp;algebra</a></div>
<div class="menu-item"><a href="ase2020.html">ASE2020&nbsp;Dynamics</a></div>
<div class="menu-item"><a href="ee787.html" class="current">EE787&nbsp;Machine&nbsp;learning</a></div>
<div class="menu-item"><a href="ee786.html">EE786&nbsp;Optimization</a></div>
<div class="menu-item"><a href="ee370.html">EE370&nbsp;Software&nbsp;lab.</a></div>
<div class="menu-item"><a href="ee363.html">EE363&nbsp;Automatic&nbsp;control</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>EE787: Machine Learning </h1>
<div id="subtitle"><a href="http://jonghank.khu.ac.kr" target=&ldquo;blank&rdquo;>Prof. Jong-Han Kim</a>, <a href="http://www.khu.ac.kr" target=&ldquo;blank&rdquo;>Kyung Hee University</a>, Autumn 2019 </div>
</div>
<div class="infoblock">
<div class="blockcontent">
<h2>Announcements</h2>
<ul>
<li><p>No class on 12/10 and 12/12, and make-up sessions on 12/20, 12/24, 12/26, and 12/27, at 10:30am-12:00pm in Electronics and Information Bldg. Rm.539.</p>
</li>
<li><p>No class on 10/29.</p>
</li>
<li><p>Homework #2 was posted.</p>
</li>
<li><p>Homework #1 was posted.</p>
</li>
<li><p>Welcome to EE787: Machine learning.</p>
</li>
</ul>
</div></div>
<h2>Course Info.</h2>
<p><b>Course descriptions</b> </p>
<ul>
<li><p>Fundamental concepts and theories in machine learning, supervised and unsupervised learning, regression and classification, loss function selection and its effect on learning, regularization and robustness to outliers, numerical experiments on data from a wide variety of engineering and other disciplines.</p>
</li>
</ul>
<p><b>Lectures</b> </p>
<ul>
<li><p>Tue/Thr 13:30-14:45 (Multimedia bldg. Rm.210)</p>
</li>
</ul>
<p><b>Office hours</b></p>
<ul>
<li><p>Tue/Thr 15:00-16:00 (Rm.516), or by appointments if you cannot meet them.</p>
</li>
</ul>
<p><b>Prerequisites</b> </p>
<ul>
<li><p>Previous exposure to linear algebra, probability, and programming.</p>
</li>
<li><p>Working knowledge on optimization will be a plus.</p>
</li>
</ul>
<p><b>Reference textbooks</b> <br /></p>
<ul>
<li><p>There are no required textbooks.</p>
</li>
<li><p><a href="http://vmls-book.org" target=&ldquo;blank&rdquo;>Introduction to applied linear algebra - vectors, matrices, and least squares</a>
by Boyd and Vandenberghe will be a useful reference.</p>
</li>
</ul>
<p><b>Grading policy</b></p>
<ul>
<li><p>No exams.</p>
</li>
<li><p>Students will be evaluated by their homework assignments.</p>
</li>
</ul>
<h2>Lecture notes</h2>
<p><i>The course material is reproduced from the <a href="http://ee104.stanford.edu" target=&ldquo;blank&rdquo;>EE104: Introduction to machine learning</a> by <a href="http://lall.stanford.edu" target=&ldquo;blank&rdquo;>Sanjay Lall</a> and <a href="http://web.stanford.edu/~boyd/" target=&ldquo;blank&rdquo;>Stephen Boyd</a> at Stanford university, under their kind permission.</i></p>
<ol>
<li><p><a href="./ee787/2019a/overview.pdf" target=&ldquo;blank&rdquo;>Course overview</a></p>
</li>
<li><p><a href="./ee787/2019a/supervised.pdf" target=&ldquo;blank&rdquo;>Supervised learning via empirical risk minimization</a></p>
</li>
<li><p><a href="./ee787/2019a/regression.pdf" target=&ldquo;blank&rdquo;>Least squares linear regression</a></p>
</li>
<li><p><a href="./ee787/2019a/validation.pdf" target=&ldquo;blank&rdquo;>Validation</a></p>
</li>
<li><p><a href="./ee787/2019a/features.pdf" target=&ldquo;blank&rdquo;>Features</a></p>
</li>
<li><p><a href="./ee787/2019a/regularization.pdf" target=&ldquo;blank&rdquo;>Regularization</a></p>
</li>
<li><p><a href="./ee787/2019a/example.pdf" target=&ldquo;blank&rdquo;>House prices example</a></p>
</li>
<li><p><a href="./ee787/2019a/losses.pdf" target=&ldquo;blank&rdquo;>Non-quadratic losses</a></p>
</li>
<li><p><a href="./ee787/2019a/regularizers.pdf" target=&ldquo;blank&rdquo;>Non-quadratic regularizers</a></p>
</li>
<li><p><a href="./ee787/2019a/optimization.pdf" target=&ldquo;blank&rdquo;>Optimization</a></p>
</li>
<li><p><a href="./ee787/2019a/prox_gradient.pdf" target=&ldquo;blank&rdquo;>Prox-gradient method</a></p>
</li>
<li><p><a href="./ee787/2019a/boolean_classification.pdf" target=&ldquo;blank&rdquo;>Boolean classification</a></p>
</li>
<li><p><a href="./ee787/2019a/multiclass.pdf" target=&ldquo;blank&rdquo;>Multi-class classification</a></p>
</li>
<li><p><a href="./ee787/2019a/neural.pdf" target=&ldquo;blank&rdquo;>Neural networks</a></p>
</li>
<li><p><a href="./ee787/2019a/unsupervised.pdf" target=&ldquo;blank&rdquo;>Unsupervised learning</a>
</p>
</li>
</ol>
<h2>Assignments</h2>
<p><i>Several sets of occasional homeworks will be assigned. 
You are encouraged to work in groups, however everyone should turn in his/her own work.
Some of these assignments are from <a href="http://vmls-book.org" target=&ldquo;blank&rdquo;>Introduction to applied linear algebra - vectors, matrices, and least squares</a>.</i></p>
<ol>
<li><p><a href="./ee787/2019a/homeworks/hw1.pdf" target=&ldquo;blank&rdquo;>Homework #1</a> (due 10/22)</p>
</li>
<li><p><a href="./ee787/2019a/homeworks/hw2.pdf" target=&ldquo;blank&rdquo;>Homework #2</a> (due 11/7)</p>
</li>
</ol>
<h2>Julia</h2>
<p><b>Julia language</b></p>
<ul>
<li><p>We will be using Julia, which excels in high performance technical computing, for homework assignments.</p>
</li>
<li><p>You are not expected to have a strong background in programming (with Julia or otherwise), because the program you will write will use only a tiny subset of Julia's (many and powerful) features.</p>
</li>
</ul>
<p><b>Reference webpages</b></p>
<ul>
<li><p><a href="http://julialang.org" target=&ldquo;blank&rdquo;>The official Julia webpage</a></p>
</li>
<li><p><a href="http://juliabox.com" target=&ldquo;blank&rdquo;>JuliaBox: an easy-to-use online platform of Julia</a></p>
</li>
<li><p><a href="http://juliacomputing.com" target=&ldquo;blank&rdquo;>A variety of Julia product packages including JuliaPro</a></p>
</li>
<li><p><a href="http://vmls-book.stanford.edu" target=&ldquo;blank&rdquo;>Julia language companion: an excellent source of Julia codes written in Julia</a></p>
</li>
</ul>
<h2>Files</h2>
<p><i>These are some data and the Julia codes in .ipynb notebook files that we are using for lectures or homework assignments.</i></p>
<ol>
<li><p><a href="./ee787/2019a/files/straight_line_fit_in_julia.html" target=&ldquo;blank&rdquo;>Straight line fit in julia</a> 
(<a href="./ee787/2019a/files/straight_line_fit_in_julia.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/straight_line_fit_in_python.html" target=&ldquo;blank&rdquo;>Straight line fit in python</a> 
(<a href="./ee787/2019a/files/straight_line_fit_in_python.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/diabetes.html" target=&ldquo;blank&rdquo;>Diabetes example</a> (<a href="./ee787/2019a/files/diabetes.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/diabetes_split.html" target=&ldquo;blank&rdquo;>Splitting dataset</a> (<a href="./ee787/2019a/files/diabetes_split.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/polynomial_fit.html" target=&ldquo;blank&rdquo;>Polynomial fit</a> (<a href="./ee787/2019a/files/polynomial_fit.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/sequential_outlier_detection.html" target=&ldquo;blank&rdquo;>Reading <tt>json</tt> files from webpages</a> (<a href="./ee787/2019a/files/sequential_outlier_detection.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/house_price_example.html" target=&ldquo;blank&rdquo;>House price example</a> (<a href="./ee787/2019a/files/house_price_example.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>, <a href="./ee787/2019a/files/train.csv" target=&ldquo;blank&rdquo;>train.csv</a>, <a href="./ee787/2019a/files/data_description.txt" target=&ldquo;blank&rdquo;>data_description.txt</a>  ), </p>
</li>
<li><p><a href="./ee787/2019a/files/tomography.html" target=&ldquo;blank&rdquo;>Tomography</a> (<a href="./ee787/2019a/files/tomography.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>, <a href="./ee787/2019a/files/line_pixel_length.jl" target=&ldquo;blank&rdquo;>line_pixel_length.jl</a>, <a href="./ee787/2019a/files/tomodata_fullysampled.json" target=&ldquo;blank&rdquo;>tomodata_fullysampled.json</a>, <a href="./ee787/2019a/files/tomodata_undersampled.json" target=&ldquo;blank&rdquo;>tomodata_undersampled.json</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/piecewise_linear_fit.html" target=&ldquo;blank&rdquo;>Piecewise-linear fit</a> (<a href="./ee787/files/piecewise_linear_fit.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/Convex_jl_tutorial.html" target=&ldquo;blank&rdquo;><tt>Convex.jl</tt> tutorial</a> (<a href="./ee787/files/Convex_jl_tutorial.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/binary_classifiers.html" target=&ldquo;blank&rdquo;>Binary classifiers</a> (<a href="./ee787/files/Binary_classifiers.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
<li><p><a href="./ee787/2019a/files/iris.html" target=&ldquo;blank&rdquo;>Iris classification</a> (<a href="./ee787/2019a/files/iris.csv" target=&ldquo;blank&rdquo;>iris.csv</a>, <a href="./ee787/files/iris.ipynb" target=&ldquo;blank&rdquo;>.ipynb</a>)</p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
Page generated 2021-02-27, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
